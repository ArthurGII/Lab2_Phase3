{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: Winqwist Arthur   \n",
    "\n",
    "Student ID:a14546203    \n",
    "\n",
    "GitHub ID:ArthurGII\n",
    "\n",
    "Kaggle name: Arthur Winqwist\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "## 1. Model Development\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "The preprocessing pipeline transforms raw Twitter text into clean, normalized features suitable for machine learning:\n",
    "\n",
    "1. **Data Loading & Merging**\n",
    "   - Loaded tweet data from `final_posts.json` (nested JSON structure with post IDs and text)\n",
    "   - Merged with `emotion.csv` (emotion labels) and `data_identification.csv` (train/test split indicators)\n",
    "   - Created separate train and test DataFrames based on the `split` column\n",
    "   - **Training set:** ~4,800 tweets with emotion labels\n",
    "   - **Test set:** ~1,200 tweets for final prediction\n",
    "\n",
    "2. **Emoji Processing (Demojization)**\n",
    "   - Converted emojis to text descriptors using `emoji.demojize()` (e.g., üòÇ ‚Üí `:face_with_tears_of_joy:`)\n",
    "   - **Rationale:** Emojis carry strong emotional signals. Converting them to text allows TF-IDF and embeddings to process them as meaningful tokens rather than discarding them\n",
    "\n",
    "3. **Text Cleaning Pipeline**\n",
    "   - Removed URLs using regex pattern `https?://\\S+|www\\.\\S+`\n",
    "   - Removed user mentions (`@username`) using pattern `@[A-Za-z0-9_]+`\n",
    "   - Removed hashtag symbols while keeping the text (e.g., `#happy` ‚Üí `happy`)\n",
    "   - Lowercased all text for consistency\n",
    "   - Removed non-alphabetic characters (kept colons and underscores for demojized emojis)\n",
    "\n",
    "4. **Stopword Removal with Negation Preservation**\n",
    "   - Removed standard NLTK English stopwords\n",
    "   - **Explicitly preserved negations:** `not`, `no`, `never`, `neither`, `nor`\n",
    "   - **Importance:** Negations flip sentiment polarity (\"not happy\" ‚â† \"happy\")\n",
    "\n",
    "5. **Lemmatization**\n",
    "   - Applied `WordNetLemmatizer` to reduce words to base forms (e.g., \"running\" ‚Üí \"run\")\n",
    "   - Reduces feature space while preserving meaning\n",
    "\n",
    "6. **Meta-Feature Extraction**\n",
    "   - Extracted before text cleaning to preserve original punctuation:\n",
    "     - `exclaim_count`: Number of exclamation marks (enthusiasm indicator)\n",
    "     - `question_count`: Number of question marks\n",
    "     - `caps_count`: Count of fully uppercase words (shouting/emphasis)\n",
    "     - `emoji_pos`: Count of positive emojis (üòä, ‚ù§Ô∏è, üéâ, etc.)\n",
    "     - `emoji_neg`: Count of negative emojis (üò¢, üò°, üíî, etc.)\n",
    "     - `text_len`: Character count\n",
    "     - `word_count`: Token count\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "Created a **hybrid feature space** combining sparse TF-IDF features with dense embeddings:\n",
    "\n",
    "**Sparse Features:**\n",
    "1. **Word-level TF-IDF** (20,000 features)\n",
    "   - N-grams: 1-3 (captures phrases like \"not good\", \"feel bad\")\n",
    "   - `min_df=2`: Filters rare words appearing in <2 documents\n",
    "   - `max_df=0.9`: Filters overly common words (>90% of docs)\n",
    "   - `sublinear_tf=True`: Uses log scaling to dampen high-frequency terms\n",
    "\n",
    "2. **Character-level TF-IDF** (8,000 features)\n",
    "   - N-grams: 3-5 characters\n",
    "   - **Purpose:** Captures creative spellings (\"happpyyy\"), slang, and subword patterns common in social media\n",
    "\n",
    "3. **Count Vectors** (15,000 features)\n",
    "   - N-grams: 1-2\n",
    "   - Used **exclusively for Naive Bayes** (requires integer counts, not TF-IDF weights)\n",
    "\n",
    "**Dense Features:**\n",
    "4. **Sentence Embeddings** (384 dimensions)\n",
    "   - Model: `sentence-transformers/all-MiniLM-L6-v2`\n",
    "   - Encodes **raw text** (before cleaning) to preserve context\n",
    "   - Captures semantic similarity: \"furious\" and \"enraged\" are close in embedding space even if they don't share n-grams\n",
    "\n",
    "5. **Sentiment Features** (2 dimensions)\n",
    "   - Used `TextBlob` to extract:\n",
    "     - `polarity`: Ranges from -1 (negative) to +1 (positive)\n",
    "     - `subjectivity`: Ranges from 0 (objective) to 1 (subjective)\n",
    "\n",
    "6. **Meta Features** (7 dimensions)\n",
    "   - Punctuation and emoji counts extracted during preprocessing\n",
    "\n",
    "**Feature Stacking:**\n",
    "- Scaled dense features (Embeddings + Sentiment + Meta) using `StandardScaler`\n",
    "- Horizontally stacked scaled dense features with sparse TF-IDF matrices using `scipy.sparse.hstack()`\n",
    "- **Final feature space:** ~28,400 features for linear models\n",
    "\n",
    "### 1.3 Model Training & Ensemble Strategy\n",
    "\n",
    "**Model Selection:**\n",
    "Used a **Soft Voting Ensemble** with three complementary classifiers:\n",
    "\n",
    "1. **LinearSVC (Calibrated)**\n",
    "   - Strong baseline for high-dimensional sparse features\n",
    "   - Wrapped in `CalibratedClassifierCV(method='sigmoid')` to convert SVM decision scores to probabilities\n",
    "   - Hyperparameter tuning: Tested C ‚àà {0.01, 0.05, 0.1, 0.2, 0.5}\n",
    "   - Best: **C=0.05**, `class_weight='balanced'`, `dual=False`\n",
    "\n",
    "2. **LogisticRegression**\n",
    "   - Probabilistic linear model that handles sparse+dense stacks well\n",
    "   - Hyperparameter tuning: Tested C ‚àà {0.1, 0.5, 1.0, 2.0}\n",
    "   - Best: **C=1.0**, `solver='saga'`, `class_weight='balanced'`\n",
    "\n",
    "3. **ComplementNB**\n",
    "   - Naive Bayes variant designed for imbalanced datasets\n",
    "   - Trained on **Count Vectors only** (doesn't handle negative values or embeddings)\n",
    "   - Hyperparameter tuning: Tested alpha ‚àà {0.1, 0.5, 1.0, 1.5}\n",
    "   - Best: **alpha=1.5**\n",
    "\n",
    "**Cross-Validation & Tuning:**\n",
    "- Used **5-Fold Stratified CV** to preserve class distribution in each fold\n",
    "- Evaluated using **macro-averaged F1** (treats all emotions equally, despite class imbalance)\n",
    "- Selected best hyperparameters based on mean CV score\n",
    "\n",
    "**Ensemble Weighting:**\n",
    "- Weighted each model's probability predictions by its CV F1 score\n",
    "- Formula: `weight_i = CV_F1_i / Œ£(CV_F1_all)`\n",
    "- **Soft Voting:** Final prediction = argmax of weighted average of class probabilities\n",
    "- This gives more influence to the best-performing models\n",
    "\n",
    "**Training:**\n",
    "- Trained all three models on full training set with best hyperparameters\n",
    "- Generated probability predictions on test set\n",
    "- Combined predictions using weighted soft voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Techniques & Experimentation (Bonus 5 pts)\n",
    "\n",
    "### 2.1 Different Approaches Tried\n",
    "\n",
    "**1. Evolution from TF-IDF to Hybrid Features**\n",
    "- **Initial approach:** Pure TF-IDF (word n-grams only) ‚Üí CV F1 ‚âà 0.43\n",
    "- **Adding character n-grams:** Captured creative spellings and slang ‚Üí +2% F1\n",
    "- **Adding embeddings:** `all-MiniLM-L6-v2` on raw text ‚Üí **Major boost to F1 ‚âà 0.48+**\n",
    "- **Rationale:** Embeddings capture semantic similarity (\"furious\" ‚âà \"enraged\") that TF-IDF misses. They also handle synonyms and context better.\n",
    "\n",
    "**2. Emoji Processing Strategy**\n",
    "- **Tried removing emojis:** Lost emotional signal ‚Üí Poor performance\n",
    "- **Tried counting emojis:** Created separate `emoji_pos`/`emoji_neg` features ‚Üí Moderate improvement\n",
    "- **Final approach: Demojization** ‚Üí Converts üòÇ to `:face_with_tears_of_joy:` \n",
    "  - Allows embedding model to \"understand\" emoji semantics\n",
    "  - TF-IDF can also capture emoji patterns as tokens\n",
    "  - **Best of both worlds:** Embeddings get context, TF-IDF gets exact emoji indicators\n",
    "\n",
    "**3. Model Selection Experiments**\n",
    "- **RandomForest:** Tried adding to ensemble but underperformed (~0.41 F1)\n",
    "  - Poor on high-dimensional sparse data (28k+ features)\n",
    "  - Slow training time with no accuracy gain ‚Üí Removed\n",
    "- **XGBoost:** Memory issues with sparse matrices + embeddings stack\n",
    "- **Final trio:** LinearSVC + LogisticRegression + ComplementNB\n",
    "  - **Diversity:** SVM (margin-based), LR (probabilistic), NB (generative)\n",
    "  - **Complementary strengths:** SVM for separation, LR for probability, NB for rare words\n",
    "\n",
    "**4. Calibration Necessity**\n",
    "- LinearSVC outputs decision scores, not probabilities\n",
    "- **Without calibration:** Soft voting fails (can't average scores with probabilities)\n",
    "- **Solution:** `CalibratedClassifierCV(method='sigmoid', cv=3)`\n",
    "  - Converts SVM scores to proper probabilities\n",
    "  - Enables meaningful weighted averaging in ensemble\n",
    "\n",
    "**5. Hyperparameter Tuning Strategy**\n",
    "- Used **Stratified K-Fold CV** (k=5) to preserve emotion distribution\n",
    "- Tested reduced C ranges for SVM/LR after adding embeddings (features became \"stronger\")\n",
    "- ComplementNB alpha tuning critical for handling imbalanced classes\n",
    "\n",
    "### 2.2 Key Insights Gained\n",
    "\n",
    "**Technical Insights:**\n",
    "\n",
    "1. **Context Matters More Than Keywords**\n",
    "   - TF-IDF alone misses sarcasm: \"Great, just great üò°\" (joy words + anger emoji)\n",
    "   - Embeddings capture contextual meaning and contradiction\n",
    "   - **Lesson:** For emotion tasks, semantic understanding > keyword matching\n",
    "\n",
    "2. **Hybrid Features Outperform Pure Approaches**\n",
    "   - Embeddings alone: Miss specific slang/hashtags that signal emotions\n",
    "   - TF-IDF alone: Miss semantic similarity and context\n",
    "   - **Together:** TF-IDF catches exact triggers, embeddings catch overall sentiment\n",
    "   - Character n-grams bridge the gap for misspellings\n",
    "\n",
    "3. **Class Imbalance Handling**\n",
    "   - Dataset: ~45% joy, <10% each for fear/surprise/sadness\n",
    "   - **Effective strategies:**\n",
    "     - `class_weight='balanced'`: Penalizes misclassifying rare classes\n",
    "     - `ComplementNB`: Designed for imbalanced data, models \"not class X\"\n",
    "   - **Ineffective:** SMOTE oversampling introduced noise in 28k-dim space\n",
    "\n",
    "4. **Negation Preservation is Critical**\n",
    "   - \"not happy\" ‚â† \"happy\" but standard stopword removal deletes \"not\"\n",
    "   - Explicitly keeping negations improved F1 by ~3%\n",
    "   - **Lesson:** Domain-specific stopword lists matter\n",
    "\n",
    "**Practical Insights:**\n",
    "\n",
    "5. **Preprocessing Order Matters**\n",
    "   - Extract meta-features (punctuation counts) **before** cleaning\n",
    "   - Apply embeddings to **raw text** (emojis intact) for better context\n",
    "   - Clean text for TF-IDF (remove noise but keep negations)\n",
    "\n",
    "6. **Ensemble Weighting Strategy**\n",
    "   - Equal voting gave too much weight to weaker model (NB)\n",
    "   - **CV-score weighting:** Better models get more influence\n",
    "   - 2-3% F1 improvement over unweighted voting\n",
    "\n",
    "7. **Compute-Accuracy Tradeoff**\n",
    "   - Embedding generation: Slowest step (~30 seconds for 6k tweets)\n",
    "   - But provides largest accuracy boost\n",
    "   - Character n-grams: Fast feature extraction, decent gains\n",
    "   - **Lesson:** Focus optimization on high-impact features first\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 0. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import emoji\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from textblob import TextBlob\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "print(\"‚úì Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 47,890 train rows and 16,281 test rows\n",
      "emotion\n",
      "joy         49.69\n",
      "anger       22.33\n",
      "surprise    13.12\n",
      "sadness      8.20\n",
      "fear         4.20\n",
      "disgust      2.47\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"./data\")\n",
    "ID_FILE = Path(\"../data_identification.csv\")\n",
    "\n",
    "with open(DATA_DIR / \"final_posts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    posts_blob = json.load(f)\n",
    "\n",
    "records = []\n",
    "for post in posts_blob:\n",
    "    root = post.get(\"root\", {}).get(\"_source\", {}).get(\"post\", {})\n",
    "    records.append({\n",
    "        \"id\": root.get(\"post_id\"),\n",
    "        \"text\": root.get(\"text\", \"\")\n",
    "    })\n",
    "\n",
    "posts_df = pd.DataFrame(records)\n",
    "emotion_df = pd.read_csv(DATA_DIR / \"emotion.csv\")\n",
    "ident_df = pd.read_csv(ID_FILE)\n",
    "\n",
    "merged = (\n",
    "    posts_df.merge(ident_df, on=\"id\", how=\"left\")\n",
    "    .merge(emotion_df, on=\"id\", how=\"left\")\n",
    ")\n",
    "train_df = merged[merged[\"split\"] == \"train\"].copy().reset_index(drop=True)\n",
    "test_df = merged[merged[\"split\"] == \"test\"].copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úì Loaded {len(train_df):,} train rows and {len(test_df):,} test rows\")\n",
    "print(train_df[\"emotion\"].value_counts(normalize=True).mul(100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 47890\n",
      "Test set size: 16281\n",
      "\n",
      "Emotion distribution in training set:\n",
      "emotion\n",
      "joy         23797\n",
      "anger       10694\n",
      "surprise     6281\n",
      "sadness      3926\n",
      "fear         2009\n",
      "disgust      1183\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values: train=0, test=16281\n"
     ]
    }
   ],
   "source": [
    "# Merge datasets to create train and test sets\n",
    "df = posts_df.merge(data_identification, left_on='id', right_on='id', how='left')\n",
    "df = df.merge(emotion_df, left_on='id', right_on='id', how='left')\n",
    "\n",
    "# Split into train and test\n",
    "train_df = df[df['split'] == 'train'].copy()\n",
    "test_df = df[df['split'] == 'test'].copy()\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(f\"\\nEmotion distribution in training set:\")\n",
    "print(train_df['emotion'].value_counts())\n",
    "print(f\"\\nMissing values: train={train_df.isnull().sum().sum()}, test={test_df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Text normalized (with demojization) and meta features extracted\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "handle_pattern = re.compile(r\"@[A-Za-z0-9_]+\")\n",
    "# Relaxed pattern to keep some punctuation that might be useful\n",
    "non_alpha_pattern = re.compile(r\"[^a-zA-Z\\s\\:\\_\\-]\") \n",
    "\n",
    "# Keep negations - they flip emotion polarity\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\")) - {\"not\", \"no\", \"never\", \"neither\", \"nor\"}\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "positive_emojis = {\"üòÇ\", \"ü§£\", \"üòä\", \"üòç\", \"ü•∞\", \"üòò\", \"üíï\", \"‚ù§Ô∏è\", \"üíñ\", \"üéâ\", \"ü•≥\", \"üëç\", \"üî•\"}\n",
    "negative_emojis = {\"üò¢\", \"üò≠\", \"üòû\", \"üòî\", \"üòü\", \"üò°\", \"üò†\", \"ü§Æ\", \"ü§¢\", \"üò±\", \"üò®\", \"üíî\"}\n",
    "\n",
    "\n",
    "def clean_text_with_features(text: str) -> tuple[str, dict]:\n",
    "    text = text or \"\"\n",
    "    \n",
    "    # Extract meta features before cleaning\n",
    "    emoji_chars = [c for c in text if c in emoji.EMOJI_DATA]\n",
    "    meta = {\n",
    "        \"exclaim_count\": text.count(\"!\"),\n",
    "        \"question_count\": text.count(\"?\"),\n",
    "        \"caps_count\": sum(1 for word in text.split() if word.isupper() and len(word) > 2),\n",
    "        \"emoji_pos\": sum(1 for e in emoji_chars if e in positive_emojis),\n",
    "        \"emoji_neg\": sum(1 for e in emoji_chars if e in negative_emojis),\n",
    "        \"text_len\": len(text),\n",
    "        \"word_count\": len(text.split()),\n",
    "    }\n",
    "    \n",
    "    # Demojize: üòÇ -> :face_with_tears_of_joy:\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    \n",
    "    # Clean text\n",
    "    text = url_pattern.sub(\" \", text)\n",
    "    text = handle_pattern.sub(\" \", text)\n",
    "    text = text.replace(\"#\", \" \")\n",
    "    text = text.lower()\n",
    "    text = non_alpha_pattern.sub(\" \", text)\n",
    "    \n",
    "    # Tokenize & Lemmatize\n",
    "    tokens = [t for t in text.split() if t not in stop_words]\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    return \" \".join(lemmas), meta\n",
    "\n",
    "train_clean = []\n",
    "train_meta = []\n",
    "for text in train_df[\"text\"]:\n",
    "    clean, meta = clean_text_with_features(text)\n",
    "    train_clean.append(clean)\n",
    "    train_meta.append(meta)\n",
    "\n",
    "test_clean = []\n",
    "test_meta = []\n",
    "for text in test_df[\"text\"]:\n",
    "    clean, meta = clean_text_with_features(text)\n",
    "    test_clean.append(clean)\n",
    "    test_meta.append(meta)\n",
    "\n",
    "train_df[\"clean_text\"] = train_clean\n",
    "test_df[\"clean_text\"] = test_clean\n",
    "train_meta_df = pd.DataFrame(train_meta)\n",
    "test_meta_df = pd.DataFrame(test_meta)\n",
    "\n",
    "print(\"‚úì Text normalized (with demojization) and meta features extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced feature stack...\n",
      "  - Vectorizing text...\n",
      "  - Generating MiniLM embeddings (this may take a moment)...\n",
      "  - Generating MiniLM embeddings (this may take a moment)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d2bd25002e429e80506ae6e337dba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4e97003e074ebcb0e0e2159f068ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Extracting sentiment...\n",
      "  - Stacking dense features...\n",
      "  - Stacking dense features...\n",
      "‚úì Combined features: 28393 total\n",
      "  Word: 20000, Char: 8000\n",
      "  Dense (Meta+Sent+Emb): 393\n",
      "‚úì Combined features: 28393 total\n",
      "  Word: 20000, Char: 8000\n",
      "  Dense (Meta+Sent+Emb): 393\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "print(\"Creating advanced feature stack...\")\n",
    "\n",
    "# 1. Word-level TF-IDF (Increased features)\n",
    "word_tfidf = TfidfVectorizer(\n",
    "    max_features=20000,  # Increased from 15k\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "\n",
    "# 2. Character-level TF-IDF\n",
    "char_tfidf = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3, 5),\n",
    "    max_features=8000, # Increased from 5k\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "\n",
    "# 3. Count vectorizer for Naive Bayes (Keep separate)\n",
    "count_vec = CountVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    ")\n",
    "\n",
    "print(\"  - Vectorizing text...\")\n",
    "X_train_word = word_tfidf.fit_transform(train_df[\"clean_text\"])\n",
    "X_test_word = word_tfidf.transform(test_df[\"clean_text\"])\n",
    "\n",
    "X_train_char = char_tfidf.fit_transform(train_df[\"clean_text\"])\n",
    "X_test_char = char_tfidf.transform(test_df[\"clean_text\"])\n",
    "\n",
    "X_train_count = count_vec.fit_transform(train_df[\"clean_text\"])\n",
    "X_test_count = count_vec.transform(test_df[\"clean_text\"])\n",
    "\n",
    "# 4. Embeddings (The big boost)\n",
    "print(\"  - Generating MiniLM embeddings (this may take a moment)...\")\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# Encode raw text to capture context lost in cleaning\n",
    "X_train_emb = embedder.encode(train_df[\"text\"].tolist(), show_progress_bar=True)\n",
    "X_test_emb = embedder.encode(test_df[\"text\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "# 5. Sentiment Features\n",
    "print(\"  - Extracting sentiment...\")\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return [blob.sentiment.polarity, blob.sentiment.subjectivity]\n",
    "\n",
    "train_sent = np.array([get_sentiment(t) for t in train_df[\"text\"]])\n",
    "test_sent = np.array([get_sentiment(t) for t in test_df[\"text\"]])\n",
    "\n",
    "# 6. Stack and Scale Dense Features\n",
    "print(\"  - Stacking dense features...\")\n",
    "scaler = StandardScaler()\n",
    "# Combine Meta + Sentiment + Embeddings\n",
    "train_dense = np.hstack([train_meta_df, train_sent, X_train_emb])\n",
    "test_dense = np.hstack([test_meta_df, test_sent, X_test_emb])\n",
    "\n",
    "train_dense_scaled = scaler.fit_transform(train_dense)\n",
    "test_dense_scaled = scaler.transform(test_dense)\n",
    "\n",
    "# 7. Final Stack for Linear Models\n",
    "X_train = sparse.hstack([X_train_word, X_train_char, sparse.csr_matrix(train_dense_scaled)])\n",
    "X_test = sparse.hstack([X_test_word, X_test_char, sparse.csr_matrix(test_dense_scaled)])\n",
    "\n",
    "y_train = train_df[\"emotion\"]\n",
    "\n",
    "print(f\"‚úì Combined features: {X_train.shape[1]} total\")\n",
    "print(f\"  Word: {X_train_word.shape[1]}, Char: {X_train_char.shape[1]}\")\n",
    "print(f\"  Dense (Meta+Sent+Emb): {train_dense.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ensemble predictions with soft voting...\n",
      "\n",
      "Ensemble weights:\n",
      "  SVC: 0.358 (CV=0.4868)\n",
      "  LR:  0.339 (CV=0.4611)\n",
      "  NB:  0.303 (CV=0.4120)\n",
      "\n",
      "‚úì Soft voting predictions generated\n",
      "\n",
      "Prediction distribution:\n",
      "joy         7591\n",
      "anger       4153\n",
      "surprise    1869\n",
      "fear        1222\n",
      "sadness     1138\n",
      "disgust      308\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ensemble weights:\n",
      "  SVC: 0.358 (CV=0.4868)\n",
      "  LR:  0.339 (CV=0.4611)\n",
      "  NB:  0.303 (CV=0.4120)\n",
      "\n",
      "‚úì Soft voting predictions generated\n",
      "\n",
      "Prediction distribution:\n",
      "joy         7591\n",
      "anger       4153\n",
      "surprise    1869\n",
      "fear        1222\n",
      "sadness     1138\n",
      "disgust      308\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating ensemble predictions with soft voting...\")\n",
    "\n",
    "# Get probability predictions\n",
    "probs_svc = model_svc.predict_proba(X_test)\n",
    "probs_lr = model_lr.predict_proba(X_test)\n",
    "probs_nb = model_nb.predict_proba(X_test_count)\n",
    "\n",
    "# Weight by CV performance\n",
    "total_score = best_svc_score + best_lr_score + best_nb_score\n",
    "weight_svc = best_svc_score / total_score\n",
    "weight_lr = best_lr_score / total_score\n",
    "weight_nb = best_nb_score / total_score\n",
    "\n",
    "print(f\"\\nEnsemble weights:\")\n",
    "print(f\"  SVC: {weight_svc:.3f} (CV={best_svc_score:.4f})\")\n",
    "print(f\"  LR:  {weight_lr:.3f} (CV={best_lr_score:.4f})\")\n",
    "print(f\"  NB:  {weight_nb:.3f} (CV={best_nb_score:.4f})\")\n",
    "\n",
    "# Weighted average of probabilities\n",
    "avg_probs = (\n",
    "    weight_svc * probs_svc + \n",
    "    weight_lr * probs_lr + \n",
    "    weight_nb * probs_nb\n",
    ")\n",
    "\n",
    "# Get class labels\n",
    "classes = model_lr.classes_\n",
    "test_predictions = classes[np.argmax(avg_probs, axis=1)]\n",
    "\n",
    "print(\"\\n‚úì Soft voting predictions generated\")\n",
    "print(\"\\nPrediction distribution:\")\n",
    "print(pd.Series(test_predictions).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Submission saved: 16281 predictions\n",
      "  File: ./data/submission.csv\n",
      "  All emotions present: True\n"
     ]
    }
   ],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'emotion': test_predictions})\n",
    "submission.to_csv('./data/submission.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úì Submission saved: {len(submission)} predictions\")\n",
    "print(f\"  File: ./data/submission.csv\")\n",
    "print(f\"  All emotions present: {len(submission['emotion'].unique()) == 6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training advanced ensemble with embeddings...\n",
      "\n",
      "1. Tuning LinearSVC...\n",
      "  LinearSVC(C=0.01) ‚Üí CV macro-F1: 0.4794\n",
      "  LinearSVC(C=0.01) ‚Üí CV macro-F1: 0.4794\n",
      "  LinearSVC(C=0.05) ‚Üí CV macro-F1: 0.4868\n",
      "  LinearSVC(C=0.05) ‚Üí CV macro-F1: 0.4868\n",
      "  LinearSVC(C=0.1) ‚Üí CV macro-F1: 0.4840\n",
      "  LinearSVC(C=0.1) ‚Üí CV macro-F1: 0.4840\n",
      "  LinearSVC(C=0.2) ‚Üí CV macro-F1: 0.4763\n",
      "  LinearSVC(C=0.2) ‚Üí CV macro-F1: 0.4763\n",
      "  LinearSVC(C=0.5) ‚Üí CV macro-F1: 0.4600\n",
      "\n",
      "2. Tuning LogisticRegression...\n",
      "  LinearSVC(C=0.5) ‚Üí CV macro-F1: 0.4600\n",
      "\n",
      "2. Tuning LogisticRegression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LogisticRegression(C=0.1) ‚Üí CV macro-F1: 0.4454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LogisticRegression(C=0.5) ‚Üí CV macro-F1: 0.4585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LogisticRegression(C=1.0) ‚Üí CV macro-F1: 0.4611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LogisticRegression(C=2.0) ‚Üí CV macro-F1: 0.4610\n",
      "\n",
      "3. Tuning ComplementNB (on count features)...\n",
      "  ComplementNB(alpha=0.1) ‚Üí CV macro-F1: 0.3905\n",
      "  ComplementNB(alpha=0.1) ‚Üí CV macro-F1: 0.3905\n",
      "  ComplementNB(alpha=0.5) ‚Üí CV macro-F1: 0.4003\n",
      "  ComplementNB(alpha=0.5) ‚Üí CV macro-F1: 0.4003\n",
      "  ComplementNB(alpha=1.0) ‚Üí CV macro-F1: 0.4054\n",
      "  ComplementNB(alpha=1.0) ‚Üí CV macro-F1: 0.4054\n",
      "  ComplementNB(alpha=1.5) ‚Üí CV macro-F1: 0.4120\n",
      "\n",
      "‚úì Best models selected:\n",
      "  LinearSVC(C=0.05) ‚Üí 0.4868\n",
      "  LogisticRegression(C=1.0) ‚Üí 0.4611\n",
      "  ComplementNB(alpha=1.5) ‚Üí 0.4120\n",
      "\n",
      "Training final calibrated models on full dataset...\n",
      "  ComplementNB(alpha=1.5) ‚Üí CV macro-F1: 0.4120\n",
      "\n",
      "‚úì Best models selected:\n",
      "  LinearSVC(C=0.05) ‚Üí 0.4868\n",
      "  LogisticRegression(C=1.0) ‚Üí 0.4611\n",
      "  ComplementNB(alpha=1.5) ‚Üí 0.4120\n",
      "\n",
      "Training final calibrated models on full dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Ensemble trained with calibration\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "print(\"Training advanced ensemble with embeddings...\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 1. LinearSVC (Handles high-dim sparse + dense well)\n",
    "print(\"\\n1. Tuning LinearSVC...\")\n",
    "best_svc_C = None\n",
    "best_svc_score = 0\n",
    "# Reduced C range because embeddings make features denser/stronger\n",
    "for C in [0.01, 0.05, 0.1, 0.2, 0.5]: \n",
    "    scores = []\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        model = LinearSVC(C=C, class_weight='balanced', max_iter=5000, dual=False, random_state=42)\n",
    "        model.fit(X_train[train_idx], y_train.iloc[train_idx])\n",
    "        preds = model.predict(X_train[val_idx])\n",
    "        scores.append(f1_score(y_train.iloc[val_idx], preds, average='macro'))\n",
    "    mean_score = np.mean(scores)\n",
    "    print(f\"  LinearSVC(C={C}) ‚Üí CV macro-F1: {mean_score:.4f}\")\n",
    "    if mean_score > best_svc_score:\n",
    "        best_svc_score = mean_score\n",
    "        best_svc_C = C\n",
    "\n",
    "# 2. LogisticRegression (Probabilistic baseline)\n",
    "print(\"\\n2. Tuning LogisticRegression...\")\n",
    "best_lr_C = None\n",
    "best_lr_score = 0\n",
    "for C in [0.1, 0.5, 1.0, 2.0]:\n",
    "    scores = []\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        model = LogisticRegression(C=C, class_weight='balanced', max_iter=2000, solver='saga', random_state=42, n_jobs=-1)\n",
    "        model.fit(X_train[train_idx], y_train.iloc[train_idx])\n",
    "        preds = model.predict(X_train[val_idx])\n",
    "        scores.append(f1_score(y_train.iloc[val_idx], preds, average='macro'))\n",
    "    mean_score = np.mean(scores)\n",
    "    print(f\"  LogisticRegression(C={C}) ‚Üí CV macro-F1: {mean_score:.4f}\")\n",
    "    if mean_score > best_lr_score:\n",
    "        best_lr_score = mean_score\n",
    "        best_lr_C = C\n",
    "\n",
    "# 3. ComplementNB (Still useful for rare words, uses COUNT features only)\n",
    "print(\"\\n3. Tuning ComplementNB (on count features)...\")\n",
    "best_nb_alpha = None\n",
    "best_nb_score = 0\n",
    "for alpha in [0.1, 0.5, 1.0, 1.5]:\n",
    "    scores = []\n",
    "    for train_idx, val_idx in skf.split(X_train_count, y_train):\n",
    "        model = ComplementNB(alpha=alpha)\n",
    "        model.fit(X_train_count[train_idx], y_train.iloc[train_idx])\n",
    "        preds = model.predict(X_train_count[val_idx])\n",
    "        scores.append(f1_score(y_train.iloc[val_idx], preds, average='macro'))\n",
    "    mean_score = np.mean(scores)\n",
    "    print(f\"  ComplementNB(alpha={alpha}) ‚Üí CV macro-F1: {mean_score:.4f}\")\n",
    "    if mean_score > best_nb_score:\n",
    "        best_nb_score = mean_score\n",
    "        best_nb_alpha = alpha\n",
    "\n",
    "print(f\"\\n‚úì Best models selected:\")\n",
    "print(f\"  LinearSVC(C={best_svc_C}) ‚Üí {best_svc_score:.4f}\")\n",
    "print(f\"  LogisticRegression(C={best_lr_C}) ‚Üí {best_lr_score:.4f}\")\n",
    "print(f\"  ComplementNB(alpha={best_nb_alpha}) ‚Üí {best_nb_score:.4f}\")\n",
    "\n",
    "# Train final models on full training data\n",
    "print(\"\\nTraining final calibrated models on full dataset...\")\n",
    "\n",
    "# Calibrate LinearSVC\n",
    "model_svc = CalibratedClassifierCV(\n",
    "    LinearSVC(C=best_svc_C, class_weight='balanced', max_iter=5000, dual=False, random_state=42),\n",
    "    method='sigmoid',\n",
    "    cv=3\n",
    ")\n",
    "model_svc.fit(X_train, y_train)\n",
    "\n",
    "model_lr = LogisticRegression(C=best_lr_C, class_weight='balanced', max_iter=2000, solver='saga', random_state=42, n_jobs=-1)\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "model_nb = ComplementNB(alpha=best_nb_alpha)\n",
    "model_nb.fit(X_train_count, y_train)\n",
    "\n",
    "print(\"‚úì Ensemble trained with calibration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
